{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Fs = 30000.0\n",
    "\n",
    "def parts(list_, indices):\n",
    "    indices = [0]+indices+[len(list_)]\n",
    "    return [list_[v:indices[k+1]] for k, v in enumerate(indices[:-1])]\n",
    "\n",
    "def RemoveSlowSequences(split,split2):\n",
    "    timefiltered_split = []\n",
    "    for i,item in enumerate(split2):\n",
    "        if item[0] == 1:\n",
    "            timefiltered_split = timefiltered_split + [split[i]]\n",
    "\n",
    "    return(timefiltered_split)\n",
    "\n",
    "def aligntofirstpokeandremovesingletransits(timesplitseqs,timesplitlatencies):\n",
    "    \n",
    "    newseqs = []\n",
    "    newlatencies = []\n",
    "    # align to first poke:\n",
    "    for index_1,fragments in enumerate(timesplitseqs):\n",
    "        current_newseqs = []\n",
    "        current_newlatencies = []\n",
    "        count = -1\n",
    "        seqs = False\n",
    "        for index_2,sequence in enumerate(fragments):\n",
    "            for index_3,transit in enumerate(sequence):\n",
    "                if not str(transit)[0] == str(transit)[1]: # remove repeat pokes\n",
    "                    if str(transit)[0] == '2':\n",
    "                        seqs = True\n",
    "                        current_newseqs = current_newseqs + [[]]\n",
    "                        current_newlatencies = current_newlatencies + [[]]\n",
    "                        count = count + 1\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]\n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "                    elif seqs == True:\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]   \n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "            seqs = False\n",
    " \n",
    "        newseqs = newseqs + [current_newseqs]\n",
    "        newlatencies = newlatencies + [current_newlatencies]\n",
    "    return(newseqs,newlatencies)\n",
    "\n",
    "def generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter):\n",
    "\n",
    "    count = counter\n",
    "    transits= []\n",
    "    trial_number= []\n",
    "    for fragment in processed_seqs:\n",
    "        count = count + 1\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for transit in sequence:\n",
    "                    trial_number = trial_number + [count]\n",
    "                    transits = transits + [transit]\n",
    "        else: ### deals with cases where there are no good transitions in a trial \n",
    "            transits = transits + ['nan']\n",
    "            trial_number = trial_number + [count]\n",
    "\n",
    "    times = []\n",
    "    for fragment in processed_latencies:\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for time in sequence:\n",
    "                    times = times + [time]\n",
    "        else:\n",
    "            times = times + ['nan']\n",
    "\n",
    "    Processesed_Transition_Latencies = pd.DataFrame({'Trial': trial_number, 'Transitions' : transits,'Latencies' : times})\n",
    "\n",
    "    return(Processesed_Transition_Latencies,count)\n",
    "\n",
    "def sequence_contains_sequence(haystack_seq, needle_seq):\n",
    "    for i in range(0, len(haystack_seq) - len(needle_seq) + 1):\n",
    "        if needle_seq == haystack_seq[i:i+len(needle_seq)]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "\n",
    "\n",
    "animal = '148_2_2_implant1'\n",
    "\n",
    "path =  r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "# path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\' + animal + '\\\\'\n",
    "\n",
    "# Set as True to create baseline files that will need to be edited manually later to select timeframe:\n",
    "# create_intervales_txt = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main data loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loops across the data structure for the given animal and uses the behavuour synchronisation file which relates preprocessed bpod data (poke times) to ephys timestamps. The output is a performance score (how well the mouse did the sequence across trials) in ephys time coordinates. This allows us to later chose an ephys time period when the animal was perfroming the task well/consitently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording1_04-14-2023\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\recording1_04-14-2023\\\\behav_sync\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(recording)\n\u001b[0;32m      5\u001b[0m current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,recording,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbehav_sync\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(current_path):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m         current_path \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_path,file) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Z:\\\\projects\\\\sequence_squad\\\\organised_data\\\\animals\\\\\\\\recording1_04-14-2023\\\\behav_sync\\\\'"
     ]
    }
   ],
   "source": [
    "for recording in os.listdir(path):\n",
    "    if not 'Store' in recording: # ignore ds store thing\n",
    "        if 'recording' in recording:\n",
    "            print(recording)\n",
    "            current_path = os.path.join(path,recording,'behav_sync') + '\\\\'\n",
    "            for file in os.listdir(current_path):\n",
    "                if 'task' in file:\n",
    "                    current_path =  os.path.join(current_path,file) + '\\\\' \n",
    "            try:         \n",
    "                sync_data =  pd.read_csv(current_path + 'Transition_data_sync.csv')\n",
    "                sync_data2 =  pd.read_csv(current_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "\n",
    "                output_path = os.path.join(path,recording,'post_process_ppseq') + '\\\\' \n",
    "\n",
    "                # these are important for concainating trials later on!\n",
    "                counter1 = -1\n",
    "                counter2 = -1\n",
    "\n",
    "                #split data by trials \n",
    "                trial_split_data = dict(tuple(sync_data.groupby('Trial_id')))\n",
    "\n",
    "                # pull out transitions and timefilter data for each trial:\n",
    "                transitions = []\n",
    "                Tfilters= [[],[]]\n",
    "                latencies = [[],[]]\n",
    "                for i in trial_split_data:\n",
    "                    transitions = transitions + [list(trial_split_data[i].loc[:,'Transition_type'])]\n",
    "                    Tfilters[0] = Tfilters[0] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_out_in'])]\n",
    "                    latencies[0] = latencies[0] +[list(trial_split_data[i].loc[:,'out_in_Latency'])]   \n",
    "                    # in in\n",
    "                    Tfilters[1] = Tfilters[1] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_in_in'])]\n",
    "                    latencies[1] = latencies[1] +[list(trial_split_data[i].loc[:,'in_in_Latency'])]    \n",
    "\n",
    "                # for each trial,remove transntions that were too long and split into reaminign time relevant fragments - but for both latency types, hence the loop\n",
    "                timesplitseqs = [[],[]]\n",
    "                for i in range(2):\n",
    "                    Tfilt = Tfilters[i] # use out to in pokes first then in in .\n",
    "                    for trial_index,time_filter in enumerate(Tfilt):\n",
    "                        start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                        split = parts(transitions[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                        del split[::2] # remove every 2nd item eg. all the transitions that were timefilter = 0 so were too long. \n",
    "                        timesplitseqs[i] = timesplitseqs[i] + [TfiltSplit]\n",
    "\n",
    "                ## do the exact same for latency - but for both latency types, hence the loop:\n",
    "                timesplitlatencies = [[],[]]\n",
    "                for i in range(2):\n",
    "                    Tfilt = Tfilters[i] \n",
    "                    latency = latencies[i]\n",
    "                    for trial_index,time_filter in enumerate(Tfilt):\n",
    "                        start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                        split = parts(latency[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                        del split[::2] # remove every 2nd item eg. all the latencies that were timefilter = 0 so were too long. \n",
    "                        timesplitlatencies[i] = timesplitlatencies[i] + [TfiltSplit]\n",
    "\n",
    "                # for fragments in each trial,sort and trim so that seqs start at initiation port poke and then remove fragments that are too short. ie. remove any transitions sequences that dont inlcude the first port or are just a single transition.\n",
    "                processed_seqs,processed_latencies = aligntofirstpokeandremovesingletransits(timesplitseqs[0],timesplitlatencies[0])  ## use  timesplitlatencies[0] for Out to in Transition times \n",
    "\n",
    "                ## generate processed transition times dataframe:\n",
    "                Processesed_Transition_Latencies_df,counter1 = generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter1)\n",
    "\n",
    "                ## determine perfect sequences and correspondng training level and shaping parameters\n",
    "                trial_perfects = []\n",
    "                T_CorrectScores = [[],[],[],[]]\n",
    "                T_RepeatScores = [[],[],[],[]]\n",
    "\n",
    "                for trial_index,fragments in enumerate(processed_seqs):\n",
    "                    perfect = []\n",
    "                    for fragment in fragments:\n",
    "                        if sequence_contains_sequence(fragment,[21, 16, 63, 37]):\n",
    "                            perfect = perfect + [1]\n",
    "                        else:\n",
    "                            perfect = perfect + [0]\n",
    "\n",
    "                    trial_perfects = trial_perfects + [perfect]  \n",
    "\n",
    "                # calculate mean for each trial:\n",
    "                perfectscore_trials = []\n",
    "                for trial in trial_perfects:\n",
    "                    if len(trial) == 0:\n",
    "                        perfectscore_trials = perfectscore_trials + [0]\n",
    "                    else:\n",
    "                        perfectscore_trials = perfectscore_trials + [np.mean(trial)]\n",
    "\n",
    "                first_p_ephys_time = sync_data2.FirstPoke_EphysTime.values\n",
    "                first_p_ephys_time = first_p_ephys_time[~np.isnan(first_p_ephys_time)]\n",
    "\n",
    "                fig, ax = plt.subplots(1, 1,figsize=(20,10))\n",
    "                ax.plot(first_p_ephys_time,convolve_movmean(perfectscore_trials,20))\n",
    "                ax.set_xlabel('trials',fontsize = 15)\n",
    "                ax.set_ylabel('performance score',fontsize = 15)\n",
    "                SaveFig('Performance score.png',output_path)\n",
    "\n",
    "                out_df = pd.DataFrame({'ephys_time' : first_p_ephys_time ,\n",
    "                                'Convolved_perfromance_score' : convolve_movmean(perfectscore_trials,20)})\n",
    "\n",
    "                out_df.to_csv(output_path + '/Performance_score.csv')\n",
    "                \n",
    "                if create_intervales_txt:\n",
    "                    if not 'Time_intervales.txt' in os.listdir(output_path):\n",
    "                        ## create a txt file with example intervals for the perfect sequences:\n",
    "                        text = 'Pre_sleep,[[0,100]]\\nPost_sleep,[[200,300]]\\nAwake,[[100,200]]'\n",
    "                        with open(output_path + 'Time_intervales.txt', 'w') as f:\n",
    "                            f.write(text)\n",
    "                \n",
    "            except:\n",
    "                print('no data for this recording')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
